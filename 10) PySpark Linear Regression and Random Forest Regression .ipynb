{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (3.1.1)\n",
      "Requirement already satisfied: py4j==0.10.9 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler,VectorIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Pyspark ML Algorithms\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-30CBPS7J:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Pyspark ML Algorithms</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x159cb7a7310>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = spark.read.csv(\"insurance_claim.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type (dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+-----+--------------------------------+------------------------------+-------------------------------+-------------+--------------+-------------+----------------------------------+----------------+----------------+----------------+----------------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+----------+\n",
      "| Id|Elevation|Aspect|Slope|Horizontal_Distance_To_Hydrology|Vertical_Distance_To_Hydrology|Horizontal_Distance_To_Roadways|Hillshade_9am|Hillshade_Noon|Hillshade_3pm|Horizontal_Distance_To_Fire_Points|Wilderness_Area1|Wilderness_Area2|Wilderness_Area3|Wilderness_Area4|Soil_Type1|Soil_Type2|Soil_Type3|Soil_Type4|Soil_Type5|Soil_Type6|Soil_Type7|Soil_Type8|Soil_Type9|Soil_Type10|Soil_Type11|Soil_Type12|Soil_Type13|Soil_Type14|Soil_Type15|Soil_Type16|Soil_Type17|Soil_Type18|Soil_Type19|Soil_Type20|Soil_Type21|Soil_Type22|Soil_Type23|Soil_Type24|Soil_Type25|Soil_Type26|Soil_Type27|Soil_Type28|Soil_Type29|Soil_Type30|Soil_Type31|Soil_Type32|Soil_Type33|Soil_Type34|Soil_Type35|Soil_Type36|Soil_Type37|Soil_Type38|Soil_Type39|Soil_Type40|Cover_Type|\n",
      "+---+---------+------+-----+--------------------------------+------------------------------+-------------------------------+-------------+--------------+-------------+----------------------------------+----------------+----------------+----------------+----------------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+----------+\n",
      "|  1|     2596|    51|    3|                             258|                             0|                            510|          221|           232|          148|                              6279|               1|               0|               0|               0|         0|         0|         0|         0|         0|         0|         0|         0|         0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          1|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|         5|\n",
      "|  2|     2590|    56|    2|                             212|                            -6|                            390|          220|           235|          151|                              6225|               1|               0|               0|               0|         0|         0|         0|         0|         0|         0|         0|         0|         0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          1|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|         5|\n",
      "|  3|     2804|   139|    9|                             268|                            65|                           3180|          234|           238|          135|                              6121|               1|               0|               0|               0|         0|         0|         0|         0|         0|         0|         0|         0|         0|          0|          0|          1|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|         2|\n",
      "|  4|     2785|   155|   18|                             242|                           118|                           3090|          238|           238|          122|                              6211|               1|               0|               0|               0|         0|         0|         0|         0|         0|         0|         0|         0|         0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          1|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|         2|\n",
      "|  5|     2595|    45|    2|                             153|                            -1|                            391|          220|           234|          150|                              6172|               1|               0|               0|               0|         0|         0|         0|         0|         0|         0|         0|         0|         0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          1|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|         5|\n",
      "|  6|     2579|   132|    6|                             300|                           -15|                             67|          230|           237|          140|                              6031|               1|               0|               0|               0|         0|         0|         0|         0|         0|         0|         0|         0|         0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          1|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|         2|\n",
      "|  7|     2606|    45|    7|                             270|                             5|                            633|          222|           225|          138|                              6256|               1|               0|               0|               0|         0|         0|         0|         0|         0|         0|         0|         0|         0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          1|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|         5|\n",
      "|  8|     2605|    49|    4|                             234|                             7|                            573|          222|           230|          144|                              6228|               1|               0|               0|               0|         0|         0|         0|         0|         0|         0|         0|         0|         0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          1|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|         5|\n",
      "|  9|     2617|    45|    9|                             240|                            56|                            666|          223|           221|          133|                              6244|               1|               0|               0|               0|         0|         0|         0|         0|         0|         0|         0|         0|         0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          1|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|         5|\n",
      "| 10|     2612|    59|   10|                             247|                            11|                            636|          228|           219|          124|                              6230|               1|               0|               0|               0|         0|         0|         0|         0|         0|         0|         0|         0|         0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          1|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|         5|\n",
      "| 11|     2612|   201|    4|                             180|                            51|                            735|          218|           243|          161|                              6222|               1|               0|               0|               0|         0|         0|         0|         0|         0|         0|         0|         0|         0|          0|          0|          0|          0|          0|          0|          0|          0|          1|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|         5|\n",
      "| 12|     2886|   151|   11|                             371|                            26|                           5253|          234|           240|          136|                              4051|               1|               0|               0|               0|         0|         0|         0|         0|         0|         0|         0|         0|         0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          1|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|         2|\n",
      "| 13|     2742|   134|   22|                             150|                            69|                           3215|          248|           224|           92|                              6091|               1|               0|               0|               0|         0|         0|         0|         0|         0|         0|         0|         0|         0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          1|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|         2|\n",
      "| 14|     2609|   214|    7|                             150|                            46|                            771|          213|           247|          170|                              6211|               1|               0|               0|               0|         0|         0|         0|         0|         0|         0|         0|         0|         0|          0|          0|          0|          0|          0|          0|          0|          0|          1|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|         5|\n",
      "| 15|     2503|   157|    4|                              67|                             4|                            674|          224|           240|          151|                              5600|               1|               0|               0|               0|         0|         0|         0|         0|         0|         0|         0|         0|         0|          0|          0|          0|          0|          0|          0|          0|          0|          1|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|         5|\n",
      "| 16|     2495|    51|    7|                              42|                             2|                            752|          224|           225|          137|                              5576|               1|               0|               0|               0|         0|         0|         0|         0|         0|         0|         0|         0|         0|          0|          0|          0|          0|          0|          0|          1|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|         5|\n",
      "| 17|     2610|   259|    1|                             120|                            -1|                            607|          216|           239|          161|                              6096|               1|               0|               0|               0|         0|         0|         0|         0|         0|         0|         0|         0|         0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          1|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|         5|\n",
      "| 18|     2517|    72|    7|                              85|                             6|                            595|          228|           227|          133|                              5607|               1|               0|               0|               0|         0|         0|         0|         0|         0|         0|         0|         0|         0|          0|          0|          0|          0|          0|          0|          0|          0|          1|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|         5|\n",
      "| 19|     2504|     0|    4|                              95|                             5|                            691|          214|           232|          156|                              5572|               1|               0|               0|               0|         0|         0|         0|         0|         0|         0|         0|         0|         0|          0|          0|          0|          0|          0|          0|          0|          0|          1|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|         5|\n",
      "| 20|     2503|    38|    5|                              85|                            10|                            741|          220|           228|          144|                              5555|               1|               0|               0|               0|         0|         0|         0|         0|         0|         0|         0|         0|         0|          0|          0|          0|          0|          0|          0|          0|          0|          1|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|         5|\n",
      "+---+---------+------+-----+--------------------------------+------------------------------+-------------------------------+-------------+--------------+-------------+----------------------------------+----------------+----------------+----------------+----------------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Elevation: string (nullable = true)\n",
      " |-- Aspect: string (nullable = true)\n",
      " |-- Slope: string (nullable = true)\n",
      " |-- Horizontal_Distance_To_Hydrology: string (nullable = true)\n",
      " |-- Vertical_Distance_To_Hydrology: string (nullable = true)\n",
      " |-- Horizontal_Distance_To_Roadways: string (nullable = true)\n",
      " |-- Hillshade_9am: string (nullable = true)\n",
      " |-- Hillshade_Noon: string (nullable = true)\n",
      " |-- Hillshade_3pm: string (nullable = true)\n",
      " |-- Horizontal_Distance_To_Fire_Points: string (nullable = true)\n",
      " |-- Wilderness_Area1: string (nullable = true)\n",
      " |-- Wilderness_Area2: string (nullable = true)\n",
      " |-- Wilderness_Area3: string (nullable = true)\n",
      " |-- Wilderness_Area4: string (nullable = true)\n",
      " |-- Soil_Type1: string (nullable = true)\n",
      " |-- Soil_Type2: string (nullable = true)\n",
      " |-- Soil_Type3: string (nullable = true)\n",
      " |-- Soil_Type4: string (nullable = true)\n",
      " |-- Soil_Type5: string (nullable = true)\n",
      " |-- Soil_Type6: string (nullable = true)\n",
      " |-- Soil_Type7: string (nullable = true)\n",
      " |-- Soil_Type8: string (nullable = true)\n",
      " |-- Soil_Type9: string (nullable = true)\n",
      " |-- Soil_Type10: string (nullable = true)\n",
      " |-- Soil_Type11: string (nullable = true)\n",
      " |-- Soil_Type12: string (nullable = true)\n",
      " |-- Soil_Type13: string (nullable = true)\n",
      " |-- Soil_Type14: string (nullable = true)\n",
      " |-- Soil_Type15: string (nullable = true)\n",
      " |-- Soil_Type16: string (nullable = true)\n",
      " |-- Soil_Type17: string (nullable = true)\n",
      " |-- Soil_Type18: string (nullable = true)\n",
      " |-- Soil_Type19: string (nullable = true)\n",
      " |-- Soil_Type20: string (nullable = true)\n",
      " |-- Soil_Type21: string (nullable = true)\n",
      " |-- Soil_Type22: string (nullable = true)\n",
      " |-- Soil_Type23: string (nullable = true)\n",
      " |-- Soil_Type24: string (nullable = true)\n",
      " |-- Soil_Type25: string (nullable = true)\n",
      " |-- Soil_Type26: string (nullable = true)\n",
      " |-- Soil_Type27: string (nullable = true)\n",
      " |-- Soil_Type28: string (nullable = true)\n",
      " |-- Soil_Type29: string (nullable = true)\n",
      " |-- Soil_Type30: string (nullable = true)\n",
      " |-- Soil_Type31: string (nullable = true)\n",
      " |-- Soil_Type32: string (nullable = true)\n",
      " |-- Soil_Type33: string (nullable = true)\n",
      " |-- Soil_Type34: string (nullable = true)\n",
      " |-- Soil_Type35: string (nullable = true)\n",
      " |-- Soil_Type36: string (nullable = true)\n",
      " |-- Soil_Type37: string (nullable = true)\n",
      " |-- Soil_Type38: string (nullable = true)\n",
      " |-- Soil_Type39: string (nullable = true)\n",
      " |-- Soil_Type40: string (nullable = true)\n",
      " |-- Cover_Type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "new_dataframe = dataframe.select(*(col(c).cast(\"float\").alias(c) for c in dataframe.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: float (nullable = true)\n",
      " |-- Elevation: float (nullable = true)\n",
      " |-- Aspect: float (nullable = true)\n",
      " |-- Slope: float (nullable = true)\n",
      " |-- Horizontal_Distance_To_Hydrology: float (nullable = true)\n",
      " |-- Vertical_Distance_To_Hydrology: float (nullable = true)\n",
      " |-- Horizontal_Distance_To_Roadways: float (nullable = true)\n",
      " |-- Hillshade_9am: float (nullable = true)\n",
      " |-- Hillshade_Noon: float (nullable = true)\n",
      " |-- Hillshade_3pm: float (nullable = true)\n",
      " |-- Horizontal_Distance_To_Fire_Points: float (nullable = true)\n",
      " |-- Wilderness_Area1: float (nullable = true)\n",
      " |-- Wilderness_Area2: float (nullable = true)\n",
      " |-- Wilderness_Area3: float (nullable = true)\n",
      " |-- Wilderness_Area4: float (nullable = true)\n",
      " |-- Soil_Type1: float (nullable = true)\n",
      " |-- Soil_Type2: float (nullable = true)\n",
      " |-- Soil_Type3: float (nullable = true)\n",
      " |-- Soil_Type4: float (nullable = true)\n",
      " |-- Soil_Type5: float (nullable = true)\n",
      " |-- Soil_Type6: float (nullable = true)\n",
      " |-- Soil_Type7: float (nullable = true)\n",
      " |-- Soil_Type8: float (nullable = true)\n",
      " |-- Soil_Type9: float (nullable = true)\n",
      " |-- Soil_Type10: float (nullable = true)\n",
      " |-- Soil_Type11: float (nullable = true)\n",
      " |-- Soil_Type12: float (nullable = true)\n",
      " |-- Soil_Type13: float (nullable = true)\n",
      " |-- Soil_Type14: float (nullable = true)\n",
      " |-- Soil_Type15: float (nullable = true)\n",
      " |-- Soil_Type16: float (nullable = true)\n",
      " |-- Soil_Type17: float (nullable = true)\n",
      " |-- Soil_Type18: float (nullable = true)\n",
      " |-- Soil_Type19: float (nullable = true)\n",
      " |-- Soil_Type20: float (nullable = true)\n",
      " |-- Soil_Type21: float (nullable = true)\n",
      " |-- Soil_Type22: float (nullable = true)\n",
      " |-- Soil_Type23: float (nullable = true)\n",
      " |-- Soil_Type24: float (nullable = true)\n",
      " |-- Soil_Type25: float (nullable = true)\n",
      " |-- Soil_Type26: float (nullable = true)\n",
      " |-- Soil_Type27: float (nullable = true)\n",
      " |-- Soil_Type28: float (nullable = true)\n",
      " |-- Soil_Type29: float (nullable = true)\n",
      " |-- Soil_Type30: float (nullable = true)\n",
      " |-- Soil_Type31: float (nullable = true)\n",
      " |-- Soil_Type32: float (nullable = true)\n",
      " |-- Soil_Type33: float (nullable = true)\n",
      " |-- Soil_Type34: float (nullable = true)\n",
      " |-- Soil_Type35: float (nullable = true)\n",
      " |-- Soil_Type36: float (nullable = true)\n",
      " |-- Soil_Type37: float (nullable = true)\n",
      " |-- Soil_Type38: float (nullable = true)\n",
      " |-- Soil_Type39: float (nullable = true)\n",
      " |-- Soil_Type40: float (nullable = true)\n",
      " |-- Cover_Type: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, isnan, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id\n",
      "Elevation\n",
      "Aspect\n",
      "Slope\n",
      "Horizontal_Distance_To_Hydrology\n",
      "Vertical_Distance_To_Hydrology\n",
      "Horizontal_Distance_To_Roadways\n",
      "Hillshade_9am\n",
      "Hillshade_Noon\n",
      "Hillshade_3pm\n",
      "Horizontal_Distance_To_Fire_Points\n",
      "Wilderness_Area1\n",
      "Wilderness_Area2\n",
      "Wilderness_Area3\n",
      "Wilderness_Area4\n",
      "Soil_Type1\n",
      "Soil_Type2\n",
      "Soil_Type3\n",
      "Soil_Type4\n",
      "Soil_Type5\n",
      "Soil_Type6\n",
      "Soil_Type7\n",
      "Soil_Type8\n",
      "Soil_Type9\n",
      "Soil_Type10\n",
      "Soil_Type11\n",
      "Soil_Type12\n",
      "Soil_Type13\n",
      "Soil_Type14\n",
      "Soil_Type15\n",
      "Soil_Type16\n",
      "Soil_Type17\n",
      "Soil_Type18\n",
      "Soil_Type19\n",
      "Soil_Type20\n",
      "Soil_Type21\n",
      "Soil_Type22\n",
      "Soil_Type23\n",
      "Soil_Type24\n",
      "Soil_Type25\n",
      "Soil_Type26\n",
      "Soil_Type27\n",
      "Soil_Type28\n",
      "Soil_Type29\n",
      "Soil_Type30\n",
      "Soil_Type31\n",
      "Soil_Type32\n",
      "Soil_Type33\n",
      "Soil_Type34\n",
      "Soil_Type35\n",
      "Soil_Type36\n",
      "Soil_Type37\n",
      "Soil_Type38\n",
      "Soil_Type39\n",
      "Soil_Type40\n",
      "Cover_Type\n"
     ]
    }
   ],
   "source": [
    "for c in new_dataframe.columns:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o498.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (LAPTOP-30CBPS7J executor driver): java.io.FileNotFoundException: File file:/C:/Users/Lenovo/Downloads/insurance_claim.csv does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.FileNotFoundException: File file:/C:/Users/Lenovo/Downloads/insurance_claim.csv does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-33940ad55981>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#checking for null ir nan type values in our columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnew_dataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwhen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misNull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnew_dataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \"\"\"\n\u001b[0;32m    483\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    485\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o498.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (LAPTOP-30CBPS7J executor driver): java.io.FileNotFoundException: File file:/C:/Users/Lenovo/Downloads/insurance_claim.csv does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.FileNotFoundException: File file:/C:/Users/Lenovo/Downloads/insurance_claim.csv does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "#checking for null ir nan type values in our columns\n",
    "new_dataframe.select([count(when(col(c).isNull(), c)).alias(c) for c in new_dataframe.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(inputCols=[\"GRE Score\", \"TOEFL Score\",\"University Rating\"], \n",
    "                  outputCols=[\"GRE Score\", \"TOEFL Score\",\"University Rating\"])\n",
    "model = imputer.fit(new_dataframe)\n",
    "\n",
    "imputed_data = model.transform(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[GRE Score: float, TOEFL Score: float, University Rating: float, SOP: float, LOR: float, CGPA: float, Research: float, Chance of Admit: float]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------------+---+---+----+--------+---------------+\n",
      "|GRE Score|TOEFL Score|University Rating|SOP|LOR|CGPA|Research|Chance of Admit|\n",
      "+---------+-----------+-----------------+---+---+----+--------+---------------+\n",
      "|        0|          0|                0|  0|  0|   0|       0|              0|\n",
      "+---------+-----------+-----------------+---+---+----+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking for null ir nan type values in our columns\n",
    "imputed_data.select([count(when(col(c).isNull(), c)).alias(c) for c in imputed_data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = imputed_data.drop('Chance of Admit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[GRE Score: float, TOEFL Score: float, University Rating: float, SOP: float, LOR: float, CGPA: float, Research: float]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's assemble our features together using vectorAssembler\n",
    "assembler = VectorAssembler( inputCols=features.columns,outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(imputed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "output= output.select(\"features\", \"Chance of Admit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(imputed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "output= output.select(\"features\", \"Chance of Admit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,test_df = output.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|            features|Chance of Admit|\n",
      "+--------------------+---------------+\n",
      "|[290.0,100.0,1.0,...|           0.47|\n",
      "|[290.0,104.0,4.0,...|           0.45|\n",
      "|[294.0,93.0,1.0,1...|           0.46|\n",
      "|[295.0,93.0,1.0,2...|           0.46|\n",
      "|[295.0,96.0,2.0,1...|           0.47|\n",
      "|[295.0,99.0,1.0,2...|           0.37|\n",
      "|[295.0,101.0,2.0,...|           0.69|\n",
      "|[296.0,99.0,2.0,2...|           0.61|\n",
      "|[296.0,101.0,1.0,...|            0.6|\n",
      "|[297.0,96.0,2.0,2...|           0.43|\n",
      "|[297.0,98.0,2.0,2...|           0.59|\n",
      "|[297.0,99.0,4.0,3...|           0.54|\n",
      "|[297.0,100.0,1.0,...|           0.52|\n",
      "|[297.0,101.0,3.0,...|           0.57|\n",
      "|[298.0,92.0,1.0,2...|           0.51|\n",
      "|[298.0,97.0,3.121...|           0.45|\n",
      "|[298.0,98.0,2.0,1...|           0.44|\n",
      "|[298.0,98.0,2.0,4...|           0.34|\n",
      "|[298.0,99.0,1.0,1...|           0.53|\n",
      "|[298.0,100.0,3.0,...|           0.58|\n",
      "+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+---------------+\n",
      "|            features|Chance of Admit|\n",
      "+--------------------+---------------+\n",
      "|[293.0,97.0,2.0,2...|           0.64|\n",
      "|[294.0,95.0,1.0,1...|           0.49|\n",
      "|[295.0,99.0,2.0,2...|           0.57|\n",
      "|[296.0,95.0,2.0,3...|           0.44|\n",
      "|[296.0,97.0,2.0,1...|           0.49|\n",
      "|[296.0,99.0,2.0,3...|           0.47|\n",
      "|[297.0,96.0,2.0,2...|           0.34|\n",
      "|[298.0,101.0,2.0,...|           0.54|\n",
      "|[300.0,99.0,1.0,3...|           0.36|\n",
      "|[300.0,102.0,3.0,...|           0.63|\n",
      "|[301.0,104.0,2.0,...|           0.68|\n",
      "|[301.0,104.0,3.0,...|           0.68|\n",
      "|[301.0,107.0,3.0,...|           0.62|\n",
      "|[302.0,99.0,1.0,2...|           0.57|\n",
      "|[302.0,99.0,3.0,2...|           0.52|\n",
      "|[304.0,100.0,2.0,...|           0.64|\n",
      "|[304.0,100.0,2.0,...|           0.63|\n",
      "|[304.0,103.0,5.0,...|           0.71|\n",
      "|[304.0,105.0,1.0,...|           0.52|\n",
      "|[304.0,105.0,2.0,...|           0.54|\n",
      "+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show()\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression(featuresCol = 'features', labelCol='Chance of Admit')\n",
    "linear_model = lin_reg.fit(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0018186171380833336,0.0028664355141501128,0.008170168538492182,-0.0016601576552002632,0.018654193850216265,0.12048031637215754,0.022763298019048573]\n",
      "Intercept: -1.29343488377689\n"
     ]
    }
   ],
   "source": [
    "print(\"Coefficients: \" + str(linear_model.coefficients))\n",
    "print(\"Intercept: \" + str(linear_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.060150\n",
      "r2: 0.823635\n"
     ]
    }
   ],
   "source": [
    "trainSummary = linear_model.summary\n",
    "print(\"RMSE: %f\" % trainSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------+--------------------+\n",
      "|         prediction|Chance of Admit|            features|\n",
      "+-------------------+---------------+--------------------+\n",
      "|0.43754052197663196|           0.46|[294.0,93.0,1.0,1...|\n",
      "|0.41925217060191566|           0.46|[295.0,93.0,1.0,2...|\n",
      "| 0.5196569410324312|            0.6|[296.0,101.0,1.0,...|\n",
      "| 0.5198416468765705|           0.59|[297.0,98.0,2.0,2...|\n",
      "| 0.5162749597886092|           0.44|[298.0,98.0,2.0,1...|\n",
      "|    0.6107152518632|           0.58|[298.0,100.0,3.0,...|\n",
      "| 0.5361568013667373|           0.54|[298.0,101.0,2.0,...|\n",
      "| 0.5997541004579252|           0.53|[298.0,101.0,4.0,...|\n",
      "|  0.671707378803112|           0.69|[298.0,105.0,3.0,...|\n",
      "| 0.6644079297638907|           0.56|[299.0,102.0,3.0,...|\n",
      "| 0.5941863124916837|           0.63|[300.0,102.0,3.0,...|\n",
      "|  0.536690875900222|           0.58|[300.0,105.0,1.0,...|\n",
      "| 0.5859121962674032|           0.67|[301.0,98.0,1.0,2...|\n",
      "|  0.636236438248946|           0.68|[301.0,99.0,3.0,2...|\n",
      "| 0.5839367006103164|           0.67|[301.0,100.0,3.0,...|\n",
      "| 0.6464583506540356|           0.68|[301.0,104.0,3.0,...|\n",
      "| 0.5517812711556738|           0.56|[302.0,99.0,2.0,1...|\n",
      "| 0.5134656348460396|           0.52|[302.0,99.0,3.0,2...|\n",
      "|  0.614733901598703|           0.64|[303.0,100.0,2.0,...|\n",
      "| 0.6487277560419351|           0.62|[303.0,102.0,3.0,...|\n",
      "+-------------------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "\n",
    "predictions = linear_model.transform(test_df)\n",
    "predictions.select(\"prediction\",\"Chance of Admit\",\"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared (R2) on test data = 0.7700269878844765\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "pred_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"Chance of Admit\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data =\", pred_evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Column features already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-134-32ee602996b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeatureIndexer\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mVectorIndexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"features\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"features\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxCategories\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark-3.0.0-bin-hadoop3.2\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    127\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mC:\\spark-3.0.0-bin-hadoop3.2\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.0.0-bin-hadoop3.2\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    316\u001b[0m         \"\"\"\n\u001b[0;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.0.0-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.0.0-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    135\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.0.0-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: requirement failed: Column features already exists."
     ]
    }
   ],
   "source": [
    "featureIndexer =VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureIndexer = featureIndexer.transform(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_indexed_data = featureIndexer.select(\"indexedFeatures\", \"Chance of Admit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, test = new_indexed_data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|     indexedFeatures|Chance of Admit|\n",
      "+--------------------+---------------+\n",
      "|[290.0,104.0,4.0,...|           0.45|\n",
      "|[293.0,97.0,2.0,2...|           0.64|\n",
      "|[294.0,95.0,1.0,1...|           0.49|\n",
      "|[295.0,93.0,1.0,2...|           0.46|\n",
      "|[295.0,99.0,1.0,2...|           0.37|\n",
      "|[295.0,99.0,2.0,2...|           0.57|\n",
      "|[295.0,101.0,2.0,...|           0.69|\n",
      "|[296.0,95.0,2.0,3...|           0.44|\n",
      "|[296.0,97.0,2.0,1...|           0.49|\n",
      "|[296.0,99.0,2.0,3...|           0.47|\n",
      "|[296.0,101.0,1.0,...|            0.6|\n",
      "|[297.0,96.0,2.0,2...|           0.43|\n",
      "|[297.0,99.0,4.0,3...|           0.54|\n",
      "|[297.0,100.0,1.0,...|           0.52|\n",
      "|[297.0,101.0,3.0,...|           0.57|\n",
      "|[298.0,97.0,3.121...|           0.45|\n",
      "|[298.0,98.0,2.0,4...|           0.34|\n",
      "|[298.0,99.0,1.0,1...|           0.53|\n",
      "|[298.0,100.0,3.0,...|           0.58|\n",
      "|[298.0,101.0,2.0,...|           0.54|\n",
      "+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|     indexedFeatures|Chance of Admit|\n",
      "+--------------------+---------------+\n",
      "|[290.0,100.0,1.0,...|           0.47|\n",
      "|[294.0,93.0,1.0,1...|           0.46|\n",
      "|[295.0,96.0,2.0,1...|           0.47|\n",
      "|[296.0,99.0,2.0,2...|           0.61|\n",
      "|[297.0,96.0,2.0,2...|           0.34|\n",
      "|[297.0,98.0,2.0,2...|           0.59|\n",
      "|[298.0,92.0,1.0,2...|           0.51|\n",
      "|[298.0,98.0,2.0,1...|           0.44|\n",
      "|[298.0,101.0,4.0,...|           0.53|\n",
      "|[298.0,105.0,3.0,...|           0.69|\n",
      "|[298.0,107.187751...|           0.46|\n",
      "|[299.0,96.0,2.0,1...|           0.54|\n",
      "|[299.0,100.0,3.0,...|           0.42|\n",
      "|[300.0,100.0,3.0,...|           0.64|\n",
      "|[300.0,105.0,1.0,...|           0.58|\n",
      "|[301.0,96.0,1.0,3...|           0.54|\n",
      "|[301.0,97.0,2.0,3...|           0.44|\n",
      "|[301.0,100.0,3.0,...|           0.67|\n",
      "|[301.0,104.0,2.0,...|           0.68|\n",
      "|[301.0,104.0,3.0,...|           0.68|\n",
      "+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_reg = RandomForestRegressor(featuresCol=\"indexedFeatures\",labelCol=\"Chance of Admit\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "indexedFeatures does not exist. Available: features, Chance of Admit",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-133-765752eb5412>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Train model.  This also runs the indexer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_forest_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark-3.0.0-bin-hadoop3.2\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    127\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mC:\\spark-3.0.0-bin-hadoop3.2\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.0.0-bin-hadoop3.2\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    316\u001b[0m         \"\"\"\n\u001b[0;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.0.0-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.0.0-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    135\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.0.0-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: indexedFeatures does not exist. Available: features, Chance of Admit"
     ]
    }
   ],
   "source": [
    "# Train model.  This also runs the indexer.\n",
    "model = random_forest_reg.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+-------------------+\n",
      "|     indexedFeatures|Chance of Admit|         prediction|\n",
      "+--------------------+---------------+-------------------+\n",
      "|[290.0,100.0,1.0,...|           0.47| 0.5094936201007437|\n",
      "|[294.0,93.0,1.0,1...|           0.46|0.48588058168662174|\n",
      "|[295.0,96.0,2.0,1...|           0.47|0.49061391507558605|\n",
      "|[296.0,99.0,2.0,2...|           0.61| 0.5740165672682737|\n",
      "|[297.0,96.0,2.0,2...|           0.34|0.48234620826912505|\n",
      "|[297.0,98.0,2.0,2...|           0.59| 0.5224304593588183|\n",
      "|[298.0,92.0,1.0,2...|           0.51| 0.5070473164399992|\n",
      "|[298.0,98.0,2.0,1...|           0.44| 0.5118082880265786|\n",
      "|[298.0,101.0,4.0,...|           0.53| 0.5917534679340165|\n",
      "|[298.0,105.0,3.0,...|           0.69| 0.6536050774059645|\n",
      "|[298.0,107.187751...|           0.46| 0.5030141599807718|\n",
      "|[299.0,96.0,2.0,1...|           0.54| 0.5097316906929543|\n",
      "|[299.0,100.0,3.0,...|           0.42| 0.5497166284245074|\n",
      "|[300.0,100.0,3.0,...|           0.64| 0.6756282067623948|\n",
      "|[300.0,105.0,1.0,...|           0.58| 0.5563177801221488|\n",
      "|[301.0,96.0,1.0,3...|           0.54| 0.5225011218739077|\n",
      "|[301.0,97.0,2.0,3...|           0.44| 0.5377533459416993|\n",
      "|[301.0,100.0,3.0,...|           0.67| 0.6274649434160084|\n",
      "|[301.0,104.0,2.0,...|           0.68| 0.5881046015496703|\n",
      "|[301.0,104.0,3.0,...|           0.68| 0.6499068808149476|\n",
      "+--------------------+---------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data =  0.05799074549599089\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"Chance of Admit\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "print (\"Root Mean Squared Error (RMSE) on test data = \",evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared (R2) on test data = 0.8353874931296166\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"Chance of Admit\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data =\", evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
